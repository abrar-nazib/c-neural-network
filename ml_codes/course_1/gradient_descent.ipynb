{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load our data set\n",
    "x_train = np.array([1.0, 2.0])   #features\n",
    "y_train = np.array([300.0, 500.0])   #target value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to calculate the cost\n",
    "def compute_cost(x: np.ndarray, y: np.ndarray, w:float, b:float):\n",
    "    m = x.shape[0]\n",
    "    cost_sum = 0\n",
    "    for i in range(m):\n",
    "        f_wb = w * x[i] + b\n",
    "        cost_sum += (f_wb - y[i])**2\n",
    "    cost = (1/(2*m)*cost_sum)\n",
    "    return cost\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Linear Model:**\n",
    "$$ f_{w, b}(x^i) = wx^i + b $$\n",
    "**Cost Function**\n",
    "$$ J(w, b) = \\frac{1}{2m}\\sum_{i=1}^{i=m}(f_{w, b}(x^i)-y^i)^2 $$\n",
    "**Gradient Descent:**\n",
    "$$ w = w - \\frac{\\partial}{\\partial w}  J(w, b)$$\n",
    "$$ b = b - \\frac{\\partial}{\\partial b}  J(w, b)$$\n",
    "*Here, $w$ and $b$ has to be updated simultaneously until converge*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Simple form of derivative:**\n",
    "$$\\frac{\\partial}{\\partial w}  J(w, b) = \\frac{1}{m}\\sum_{i=1}^{i=m}\\{f_{w, b}(x^i)-y^i\\}x^i$$\n",
    "$$\\frac{\\partial}{\\partial b}  J(w, b) = \\frac{1}{m}\\sum_{i=1}^{i=m}\\{f_{w, b}(x^i)-y^i\\}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute Gradient\n",
    "def compute_gradient(x: np.ndarray, y: np.ndarray, w: float, b: float):\n",
    "    \"\"\"\n",
    "    Computes the gradient for linear regression\n",
    "    Args:   \n",
    "        x (ndarray (m, )): Training input\n",
    "        y (ndarray (m, )): Training output\n",
    "        w (scalar): Weight\n",
    "        b (scalar): bias\n",
    "    Returns:\n",
    "        dj_dw (scalar): Derivative term for weight.\n",
    "        dj_d  (scalar): Derivative term for bias.\n",
    "    \"\"\"\n",
    "    m = x.shape[0]\n",
    "    dj_dw = 0\n",
    "    dj_db = 0\n",
    "    for i in range(m):\n",
    "        f_wb = w * x[i] + b\n",
    "        dj_dw += (f_wb - y[i]) * x[i]\n",
    "        dj_db += (f_wb - y[i])\n",
    "    dj_db = dj_db / m\n",
    "    dj_dw = dj_dw / m\n",
    "    return dj_dw, dj_db\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'function' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgradient_descent\u001b[39m(x: np\u001b[39m.\u001b[39mndarray, y:np\u001b[39m.\u001b[39mndarray, w_in: \u001b[39mfloat\u001b[39m, b_in: \u001b[39mfloat\u001b[39m, alpha: \u001b[39mfloat\u001b[39m, num_iters: \u001b[39mint\u001b[39m, cost_function: function, gradient_function:function):\n\u001b[0;32m      2\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39m    Performs gradient descent to determine the suitable value of the w and b by updating them simultaneously\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[39m        p_history (List): History of parameters (w, b)\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m     18\u001b[0m     j_history \u001b[39m=\u001b[39m []\n",
      "\u001b[1;31mNameError\u001b[0m: name 'function' is not defined"
     ]
    }
   ],
   "source": [
    "def gradient_descent(x: np.ndarray, y:np.ndarray, w_in: float, b_in: float, alpha: float, num_iters: int, cost_function, gradient_function):\n",
    "    \"\"\"\n",
    "    Performs gradient descent to determine the suitable value of the w and b by updating them simultaneously\n",
    "    Args:\n",
    "        x (ndarray (m,)): Training input\n",
    "        y (ndarray (m,)): Training output\n",
    "        w_in, b_in (scalar): Initial model parameter values. Weight and Bias\n",
    "        alpha (float): Learning rate\n",
    "        num_iters (int): Number of iterations to perform gradient descent\n",
    "        cost_function: Function to call produce cost\n",
    "        gradient_function: Function to calculate derivative\n",
    "    Returns:\n",
    "        w (scalar): Updated value of weight.\n",
    "        b (scalar): Updated value of bias.\n",
    "        j_history (List): History of cost values\n",
    "        p_history (List): History of parameters (w, b)\n",
    "    \"\"\"\n",
    "    j_history = []\n",
    "    p_history = []\n",
    "    w = w_in\n",
    "    b = b_in\n",
    "    for i in range(num_iters):\n",
    "        dj_dw, dj_db = compute_gradient(x, y, w, b)\n",
    "        w = w - alpha * dj_dw\n",
    "        b = b - alpha * dj_db\n",
    "        j_history.append(cost_function(x, y, w, b))\n",
    "        p_history.append([w, b])\n",
    "        if i % math.ceil(num_iters/10):\n",
    "            print(f\"Iteration {i:4}: Cost {j_history[-1]:0.2e} \",\n",
    "                  f\"dj_dw: {dj_dw: 0.3e}, dj_db: {dj_db: 0.3e}  \",\n",
    "                  f\"w: {w: 0.3e}, b:{b: 0.5e}\")\n",
    "    return w, b, j_history, p_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing gradient descent function\n",
    "\n",
    "# Initialize parameters\n",
    "w_init = 0\n",
    "b_init = 0\n",
    "iterations = 10000\n",
    "tmp_alpha = 1.0e-2\n",
    "# Run Gradient descent with these parameters\n",
    "w_final, b_final, j_history, p_history = gradient_descent(x_train, y_train, w_init, b_init, tmp_alpha, iterations, compute_cost, compute_gradient)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c6cd0b575fc0755e990455c55bafb95cd503212c1209f644472f7d8844f98f6e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
