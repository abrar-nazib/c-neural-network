{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import copy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Dataset\n",
    "\n",
    "| Size (sqft) | Number of Bedrooms  | Number of floors | Age of  Home | Price (1000s dollars)  |   \n",
    "| ----------------| ------------------- |----------------- |--------------|-------------- |  \n",
    "| 2104            | 5                   | 1                | 45           | 460           |  \n",
    "| 1416            | 3                   | 2                | 40           | 232           |  \n",
    "| 852             | 2                   | 1                | 35           | 178           |  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 4)\n"
     ]
    }
   ],
   "source": [
    "X_train = np.array([\n",
    "    [2104, 5, 1, 45],\n",
    "    [1416, 3, 2, 40],\n",
    "    [852, 2, 1, 35]\n",
    "])\n",
    "y_train = np.array([460, 232, 178])\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_init = 785.1811367994083\n",
    "w_init = np.array([ 0.39133535, 18.75376741, -53.36032453, -26.42131618])\n",
    "# print(f\"w_init shape: {w_init.shape}, b_init type: {type(b_init)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_single_loop(x: np.ndarray, w: np.ndarray, b: float):\n",
    "    \"\"\"\n",
    "    Predicts output for a training input of multiple linear regression using looping method\n",
    "    Args:\n",
    "        x (ndarray (n, )): Input vector\n",
    "        w (ndarray (n, )): Weight Vector. Model parameter\n",
    "        b (scalar): Bias. Model parameter\n",
    "    Returns:\n",
    "        p (scalar): Prediction (house price for this case)\n",
    "    \"\"\"\n",
    "    p = 0\n",
    "    n = x.shape[0] # we are expecting 1 dimentional array\n",
    "    for i in range(n):\n",
    "        p += w[i] * x[i]\n",
    "    p += b\n",
    "    return p\n",
    "\n",
    "def predict_single_dot(x: np.ndarray, w: np.ndarray, b:float):\n",
    "    \"\"\"\n",
    "    Predicts output for a training input of multiple linear regression using dot product\n",
    "    Args:\n",
    "        x (ndarray (n, )): Input vector\n",
    "        w (ndarray (n, )): Weight Vector. Model parameter\n",
    "        b (scalar): Bias. Model parameter\n",
    "    Returns:\n",
    "        p (scalar): Prediction (house price for this case)\n",
    "    \"\"\"\n",
    "    p = np.dot(w, x) + b\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "459.9999976194083\n",
      "459.99999761940825\n"
     ]
    }
   ],
   "source": [
    "x_vec = X_train[0]\n",
    "f_wb = predict_single_loop(x_vec, w_init, b_init)\n",
    "print(f_wb)\n",
    "f_wb = predict_single_dot(x_vec, w_init, b_init)\n",
    "print(f_wb)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computation of cost with multiple variables\n",
    "Cost Function for Multiple variables:\n",
    "$$ J(\\vec{w}, b) = \\frac{1}{2m}\\sum_{i=1}^{i=m}\\left(f_{\\vec{w}, b}(\\vec{x}^i) - y^i\\right)^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(X: np.ndarray, y: np.ndarray, w: np.ndarray, b: float):\n",
    "    \"\"\"\n",
    "    Compute cost for whole training dataset\n",
    "    Args:\n",
    "        X (ndarray (m, n)): Training input matrix\n",
    "        y (ndarray (m, )): Training output vector\n",
    "        w (ndarray (n, )): Weights\n",
    "        b (scalar): Bias. Model Parameter\n",
    "    Returns:\n",
    "        cost (scalar): Cost of the model\n",
    "    \"\"\"\n",
    "\n",
    "    cost_i = 0\n",
    "    cost = 0\n",
    "    m = X.shape[0]\n",
    "    for i in range(m):\n",
    "        f_wb_i = np.dot(w, X[i]) + b\n",
    "        cost_i = (f_wb_i - y[i])**2\n",
    "        cost += cost_i\n",
    "    cost = cost/(2*m)\n",
    "    return cost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost: 1.5578904880036537e-12\n"
     ]
    }
   ],
   "source": [
    "# Compute cost using pre-chosen parameters\n",
    "cost = compute_cost(X_train, y_train, w_init, b_init)\n",
    "print(f\"Cost: {cost}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent for Multiple Variables\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{align}\n",
    "  & w_j = w_j - \\alpha \\frac{\\partial}{\\partial w_j}J(\\vec{w}, b) \\\\\n",
    "  & \\frac{\\partial}{\\partial w_j}J(\\vec{w}, b) = \\frac{1}{m}\\sum_{i=1}^{i=m}\\left(f_{\\vec{w}, b}(\\vec{x}^i) - y^i\\right)x^{i}_j \\\\\n",
    "  & b = b - \\alpha \\frac{\\partial}{\\partial b}J(\\vec{w}, b) \\\\\n",
    "  & \\frac{\\partial}{\\partial b} J(\\vec{w}, b) = \\frac{1}{m}\\sum_{i=1}^{i=m}\\left(f_{\\vec{w}, b}(\\vec{x}^i) - y^i\\right)\n",
    "  \\end{align}\n",
    "  \\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(X: np.ndarray, y: np.ndarray, w: np.ndarray, b: float): \n",
    "    \"\"\"\n",
    "    Computes the gradient for linear regression. Gradient is the partial derivative term\n",
    "    Args:\n",
    "        X (ndarray (m, n)): Training input matrix\n",
    "        y (ndarray (m, )): Training output vector\n",
    "        w (ndarray (n, )): Model parameter, weight vector\n",
    "        b (scalar): Model parameter, bias\n",
    "    Returns:\n",
    "        dj_dw (ndarray (n, )): Vector of derivative terms or gradient of weights\n",
    "        dj_db (scalar): Gradient term of bias\n",
    "    \"\"\"\n",
    "\n",
    "    m, n = X.shape\n",
    "    dj_db = 0\n",
    "    dj_dw_i = 0\n",
    "    dj_dw = np.zeros(shape=(n, ))\n",
    "    for i in range(m):\n",
    "        err = (np.dot(X[i], w) + b) - y[i]\n",
    "        for j in range(n):\n",
    "            dj_dw[j] += err * X[i, j]\n",
    "        dj_db +=  err\n",
    "    dj_dw = dj_dw / m\n",
    "    dj_db = dj_db / m\n",
    "    return (dj_dw, dj_db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dj_dw: [-2.72623581e-03 -6.27197272e-06 -2.21745580e-06 -6.92403399e-05]\n",
      "dj_db: -1.673925169143331e-06\n"
     ]
    }
   ],
   "source": [
    "tmp_dj_dw, tmp_dj_db = compute_gradient(X_train, y_train, w_init, b_init)\n",
    "print(f\"dj_dw: {tmp_dj_dw}\")\n",
    "print(f\"dj_db: {tmp_dj_db}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X: np.ndarray, y:np.ndarray, w_in:np.ndarray, b_in:float, cost_function, gradient_function, alpha:float, num_iters:int):\n",
    "    \"\"\"\n",
    "    Performes gradient descent for a multi linear regression. Updates the values of weights and bias.\n",
    "    Args:\n",
    "        X (ndarray (m, n))   : Training input matrix\n",
    "        y (ndarray (m, ))    : Training output vector\n",
    "        w_in (ndarray (n, )) : Initial weights vector\n",
    "        b_in (scalar)        : Initial bias value\n",
    "        cost_function        : Function to calculate cost of the whole dataset\n",
    "        gradient_function    : Function to calculate gradient of a row\n",
    "        alpha (float)        : The Learning Rate\n",
    "        num_iters (int)      : Number of iterations to run gradient descent\n",
    "    Returns:\n",
    "        w (ndarray (n, ))    : Updated weights vector\n",
    "        b (scalar)           : Updated bias vector   \n",
    "    \"\"\"\n",
    "    J_history = []\n",
    "    w = copy.deepcopy(w_in)\n",
    "    b = b_in\n",
    "    for i in range(num_iters):\n",
    "        # calculation of the gradients\n",
    "        dj_dw, dj_db = gradient_function(X, y, w, b)\n",
    "\n",
    "        # update the parameters\n",
    "        w = w - alpha * dj_dw       # Vector arithmatic on numpy arrays\n",
    "        b = b - alpha * dj_db       # Normal Arighmatic\n",
    "\n",
    "        cost = cost_function(X, y, w, b)\n",
    "        J_history.append(cost)\n",
    "\n",
    "        if i % math.ceil(num_iters/10) == 0:\n",
    "            print(f\"Iteration: {i}: Cost: {J_history[-1]:0.6f}\")\n",
    "\n",
    "    return w, b, J_history \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0: Cost: 2529.462952\n",
      "Iteration: 1000000: Cost: 455.074773\n",
      "Iteration: 2000000: Cost: 359.074392\n",
      "Iteration: 3000000: Cost: 283.325789\n",
      "Iteration: 4000000: Cost: 223.556746\n",
      "Iteration: 5000000: Cost: 176.396292\n",
      "Iteration: 6000000: Cost: 139.184580\n",
      "Iteration: 7000000: Cost: 109.822872\n",
      "Iteration: 8000000: Cost: 86.655168\n",
      "Iteration: 9000000: Cost: 68.374812\n",
      "w: [  0.18173972  16.29622198 -45.62523857   0.76800375]\n",
      "b:0.843349804325403\n"
     ]
    }
   ],
   "source": [
    "# Testing the gradient descent funtion\n",
    "\n",
    "# Input Variables\n",
    "initial_w = np.zeros_like(w_init)\n",
    "initial_b = 0.00\n",
    "iterations = 10000000\n",
    "alpha = 5.0e-7\n",
    "\n",
    "# Running the gradient_descent\n",
    "w_final, b_final, J_hist = gradient_descent(X_train, y_train, initial_w, initial_b, compute_cost, compute_gradient, alpha, iterations)\n",
    "print(f\"w: {w_final}\\nb:{b_final}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction 0: 426.18530497189204 for 460\n",
      "Prediction 1: 286.1674720078562 for 232\n",
      "Prediction 2: 171.46763087132317 for 178\n"
     ]
    }
   ],
   "source": [
    "# Test the prediction accuracy\n",
    "m, _ = X_train.shape\n",
    "for i in range(m):\n",
    "    print(f\"Prediction {i}: {np.dot(X_train[i], w_final) + b_final} for {y_train[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction 0: 453.63976106696657 for 460\n",
      "Prediction 1: 246.54513236734462 for 232\n",
      "Prediction 2: 169.5329280675303 for 178\n"
     ]
    }
   ],
   "source": [
    "# Test the prediction accuracy\n",
    "m, _ = X_train.shape\n",
    "for i in range(m):\n",
    "    print(f\"Prediction {i}: {np.dot(X_train[i], w_final) + b_final} for {y_train[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c6cd0b575fc0755e990455c55bafb95cd503212c1209f644472f7d8844f98f6e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
